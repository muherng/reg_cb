{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "# mosek only needed if we don't use MW\n",
    "#import mosek\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cProfile\n",
    "#from baselines import *\n",
    "from scipy.linalg import sqrtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#synthetic example with N datapoints, in d dimensions, with k actions, random regressors\n",
    "#returns a dictionary of covariates x, labels y, regressors ell.  \n",
    "def synthetic_example(N,d,k):\n",
    "    #covariates\n",
    "    x = np.random.normal(0,1,(N,d))\n",
    "    #regressors\n",
    "    ell = np.random.normal(0,1,(k,d))\n",
    "    #labels\n",
    "    y = np.zeros((N,k))\n",
    "    for i in range(N):\n",
    "        for j in range(k):\n",
    "            feature = x[i,:]\n",
    "            regressor = ell[j,:]\n",
    "            y[i,j] = np.inner(feature,regressor) + np.random.normal(0,1)\n",
    "    data = {'cov': x, 'label': y, 'reg': ell}\n",
    "    return data\n",
    "\n",
    "from scipy.special import logit, expit\n",
    "\n",
    "def logistic_synthetic_example(N,d,k,corrupt=False):\n",
    "    #covariates, scaling the covariates to be of large norm amplifies the effect of corruptions\n",
    "    x = np.random.normal(0,1,(N,d))\n",
    "    #regressors\n",
    "    ell = np.random.normal(0,1,(k,d))\n",
    "    ell = ell/np.linalg.norm(ell)\n",
    "    #labels\n",
    "    y = np.zeros((N,k))\n",
    "    prob_list = np.zeros((N,k))\n",
    "    for i in range(N):\n",
    "        for j in range(k):\n",
    "            feature = x[i,:]\n",
    "            regressor = ell[j,:]\n",
    "            prob = expit(np.inner(feature,regressor))\n",
    "            prob_list[i,j] = prob\n",
    "            #y[i,j] = np.random.binomial(1,prob)\n",
    "            #deterministic labels\n",
    "            if prob > 0.5:\n",
    "                y[i,j] = 1\n",
    "            else:\n",
    "                y[i,j] = 0\n",
    "    corr_frac = 0.1\n",
    "    corr = int(corr_frac*N)\n",
    "    print('number of corruptions')\n",
    "    print(corr)\n",
    "    if corrupt:\n",
    "        select = np.zeros((corr,k))\n",
    "        for j in range(k):\n",
    "            order = np.argsort(prob_list[:,j])\n",
    "            select[:,j] = order[:corr]\n",
    "        for i in range(corr):\n",
    "            for j in range(k):\n",
    "                index = int(select[i,j])\n",
    "                y[index,j] = 1 - y[index,j]\n",
    "    data = {'cov': x, 'label': y, 'reg': ell}\n",
    "    return data\n",
    "\n",
    "#data = synthetic_example(1000,100,10)    \n",
    "#print(data)\n",
    "\n",
    "import pandas as pd, numpy as np, re\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "\n",
    "def parse_data(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        infoline = f.readline()\n",
    "        infoline = re.sub(r\"^b'\", \"\", str(infoline))\n",
    "        n_features = int(re.sub(r\"^\\d+\\s(\\d+)\\s\\d+.*$\", r\"\\1\", infoline))\n",
    "        features, labels = load_svmlight_file(f, n_features=n_features, multilabel=True)\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    labels = mlb.fit_transform(labels)\n",
    "    features = np.array(features.todense())\n",
    "    features = np.ascontiguousarray(features)\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "data_mode = 'logistic_synth'\n",
    "#data_mode = 'synthetic'\n",
    "#data_mode = 'real'\n",
    "N = 1000\n",
    "d = 2\n",
    "k = 1\n",
    "\n",
    "if data_mode == 'logistic_synth':\n",
    "    #data = logistic_synthetic_example(N,d,k,corrupt=True)\n",
    "    data = logistic_synthetic_example(N,d,k,corrupt=False)\n",
    "if data_mode == 'synthetic':\n",
    "    data = synthetic_example(1000,10,10)    \n",
    "elif data_mode == 'real': \n",
    "    x, y = parse_data(\"Bibtex_data.txt\")\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    data = {'cov': x, 'label': y}\n",
    "\n",
    "x = data['cov']\n",
    "y = data['label']\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(solver='liblinear', random_state=0, fit_intercept ='False')\n",
    "#a = np.zeros((2,3)).reshape(-1,1)\n",
    "#b = np.ones(2)\n",
    "\n",
    "#a = np.zeros(10,2).reshape(-1, 1)\n",
    "#b = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\n",
    "lab = y[:,0]\n",
    "model.fit(x, lab)\n",
    "print(model.coef_)\n",
    "print(data['reg'])\n",
    "print('score')\n",
    "print(model.score(x,lab))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression experiments\n",
    "#implement the weighted logistic regression\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import InputLayer\n",
    "from scipy.special import expit,logit\n",
    "\n",
    "#line of code to enable the numpy operations in the custom loss\n",
    "#to be replaced\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "x_train = data['cov']\n",
    "y_train = y[:,0]\n",
    "\n",
    "\n",
    "ANN_model = Sequential()\n",
    "ANN_model.add(InputLayer(input_shape=(d, )))\n",
    "# No hidden layers\n",
    "ANN_model.add(Dense(1, activation='sigmoid',use_bias=False))\n",
    "ANN_model.summary()\n",
    "\n",
    "b_size = 10\n",
    "weights = tf.ones(b_size)\n",
    "\n",
    "def custom_loss_function(y_true, y_pred):\n",
    "    if y_true.shape[0] == None:\n",
    "        print('NONE SHAPE')\n",
    "        sq = tf.math.squared_difference(y_true,y_pred)\n",
    "        output = tf.reduce_mean(sq,axis=-1)\n",
    "    else: \n",
    "        #print('loss loop')\n",
    "        #t=1\n",
    "        #for i in range(10):\n",
    "        #    t = t+1\n",
    "        dim = y_true.shape[0]\n",
    "        y_true_copy = y_true.numpy()\n",
    "        offset = 0\n",
    "        for i in range(N-dim):\n",
    "            flag = True\n",
    "            for j in range(dim):\n",
    "                if y_train[i+j] != y_true_copy[j]:\n",
    "                    flag = False\n",
    "            if flag:\n",
    "                offset = i\n",
    "                break\n",
    "        print(offset)\n",
    "        arg1 = y_true[:,0]\n",
    "        arg2 = tf.math.log(y_pred[:,0])\n",
    "        arg3 = tf.subtract(tf.ones(dim),arg1)\n",
    "        arg4 = tf.subtract(tf.ones(dim),arg2)\n",
    "        inp1 = tf.multiply(arg1,arg2)\n",
    "        inp2 = tf.multiply(arg3,arg4)\n",
    "        loss_vec = tf.math.scalar_mul(-1.0,tf.add(inp1,inp2))\n",
    "        weight_loss_vec = tf.multiply(weights,loss_vec)\n",
    "        output = tf.reduce_mean(weight_loss_vec, axis=-1)\n",
    "    return output\n",
    "\n",
    "opt=tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "#loss=tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "#ANN_model.compile(optimizer=opt,\n",
    "#                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "#                  metrics=['accuracy'])\n",
    "\n",
    "ANN_model.compile(optimizer=opt,\n",
    "                  loss=custom_loss_function,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = ANN_model.fit(x_train, y_train, \n",
    "                        epochs=10, batch_size=b_size,\n",
    "                        validation_split=0.2, \n",
    "                        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arg = tf.constant(x_train[0,:])\n",
    "#arg = tf.ones((1,100))\n",
    "#print(arg.shape)\n",
    "#print('prediction: ',ANN_model.predict(arg))\n",
    "\n",
    "#g = ANN_model.get_layer(index=0).get_weights()\n",
    "#from scipy.special import expit,logit \n",
    "#arg = np.ones(100)\n",
    "#h = g[0][:,0]\n",
    "#print(h.shape)\n",
    "#print(np.inner(arg,arg))\n",
    "#print(arg.shape)\n",
    "\n",
    "#print(np.inner(h,h))\n",
    "#print('prediction: ', expit(np.inner(h,arg)))\n",
    "\n",
    "#b = ANN_model.get_layer(index=0).name\n",
    "#print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression version of scram\n",
    "from scipy.linalg import eigh\n",
    "\n",
    "def logistic_reg_oracle(x,y,mode='robust-logistic'):\n",
    "    if mode == 'robust-logistic':\n",
    "        lr = 0.5\n",
    "        lam = 0.2\n",
    "        MW_steps = 10\n",
    "        eta = 0.1\n",
    "        AM_steps = 10\n",
    "        params = (AM_steps,lr,lam,MW_steps,eta)\n",
    "        print('enerting logistic scram')\n",
    "        regressor = logistic_scram(x,y,params)\n",
    "    return regressor\n",
    "\n",
    "def logistic_scram(x,y,params):\n",
    "    AM_steps = params[0]\n",
    "    altmin_params = params[1:]\n",
    "    N,d = x.shape\n",
    "    w = [0.]*d\n",
    "    a = [1.]*N\n",
    "    #TODO: fix isotropic step, for now identity covariance\n",
    "    #iso_x, Sig_sqrt = isotropic(x)\n",
    "    iso_x = x\n",
    "    Sig_sqrt = np.eye(d)\n",
    "    AM_steps = 1\n",
    "    for i in range(AM_steps):\n",
    "        #print('AM step: ',i)\n",
    "        print('entering logistic altmin step')\n",
    "        w,a = logistic_altmin_step(iso_x,y,a,altmin_params)\n",
    "    final_w = np.matmul(Sig_sqrt,w)    \n",
    "    return final_w\n",
    "\n",
    "def logistic_altmin_step(Xs,Ys,a,params):\n",
    "    N,d = Xs.shape\n",
    "    w = weighted_logistic_reg(Xs,Ys,a)\n",
    "    a = logistic_get_weights(Xs,Ys,w,params)\n",
    "    return w, a\n",
    "\n",
    "def weighted_logistic_reg(x,y,a):\n",
    "    ANN_model = Sequential()\n",
    "    ANN_model.add(InputLayer(input_shape=(d, )))\n",
    "    # No hidden layers\n",
    "    ANN_model.add(Dense(1, activation='sigmoid'))\n",
    "    ANN_model.summary()\n",
    "\n",
    "    weights = tf.constant(a)\n",
    "    #weights = tf.ones(d)\n",
    "    \n",
    "    def custom_loss_function(y_true, y_pred):\n",
    "        arg1 = y_true\n",
    "        arg2 = tf.math.log(y_pred)\n",
    "        arg3 = tf.subtract(tf.ones(d),y_true)\n",
    "        arg4 = tf.subtract(tf.ones(d),y_pred)\n",
    "        inp1 = tf.multiply(arg1,arg2)\n",
    "        inp2 = tf.multiply(arg3,arg4)\n",
    "        loss_vec = tf.math.scalar_mul(-1.0,tf.add(inp1,inp2))\n",
    "        weight_loss_vec = tf.multiply(weights,loss_vec)\n",
    "        output = tf.reduce_mean(weight_loss_vec, axis=-1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    opt=tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "    ANN_model.compile(optimizer=opt,\n",
    "                  loss=custom_loss_function,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    ANN_model.fit(x_train, y_train, \n",
    "                    epochs=20, batch_size=32,\n",
    "                    validation_split=0.2, \n",
    "                    shuffle=True)\n",
    "    \n",
    "    g = ANN_model.get_layer(index=0).get_weights() \n",
    "    regressor = g[0][:,0]\n",
    "    return regressor\n",
    "\n",
    "def logistic_get_weights(X,y,w,params):\n",
    "    n,d = X.shape\n",
    "    lr,lam,steps,eta = params\n",
    "    a = np.ones(n)/float(n)\n",
    "    #Sig = np.cov(X.T)\n",
    "    Sig = np.matmul(X.T,X)\n",
    "    resids_sq = (y - np.matmul(X,w))**2\n",
    "    print('y: ', y.shape)\n",
    "    print('X: ', X.shape)\n",
    "    print('w: ', w.shape)\n",
    "    print('Xw:', np.matmul(X,w).shape)\n",
    "\n",
    "    #print(\"begin MW\")\n",
    "    for j in range(steps):\n",
    "        if j % 10 == 0:\n",
    "            print(j)\n",
    "        if lam > 0:\n",
    "            v = v_step(a,X,Sig,eta)\n",
    "        else:\n",
    "            v = np.zeros(d)\n",
    "        a = a_step(a,X,resids_sq,v,lr=lr,lam=lam,eta=eta,n=n)\n",
    "    return a\n",
    "\n",
    "def outerprod(a,X):\n",
    "    \"\"\" Compute X^T diag(a) X \"\"\"\n",
    "    left = np.multiply(X.T,a)\n",
    "    return np.dot(left,X)\n",
    "\n",
    "def cap(a,m):\n",
    "    if np.max(a) <= 1./m:\n",
    "        return a\n",
    "    #sorted_a = np.sort(a)\n",
    "    ## BUG FIX: use -a to get descending order\n",
    "    sort_ids = np.argsort(-a)\n",
    "\n",
    "    # Faster code\n",
    "    Z = np.sum(a)\n",
    "    for i in range(1,m + 1):\n",
    "        Z -= a[sort_ids[i - 1]]\n",
    "        aprime_next = (m - i) * a[sort_ids[i]]/float(m * Z)\n",
    "        if aprime_next <= 1.0/m:\n",
    "            aprime = np.copy(a)\n",
    "            aprime[sort_ids[:i]] = 1./m\n",
    "            aprime[sort_ids[i:]] *= (m - i)/float(m * Z)\n",
    "            return aprime\n",
    "\n",
    "def v_step(a,X,Sig,eta,tol=0):\n",
    "    \"\"\" Compute top eigenvector of X^T diag(1/n-a) X\"\"\"\n",
    "    # Bug fix: this was formerly 1 and not 1/n\n",
    "    n = a.shape[0]\n",
    "    #m = int(n*(1 - eta)) \n",
    "    M = outerprod(1./n-a,X)\n",
    "\n",
    "    # this method is slower sometimes and faster sometimes vs other one, depending on d?\n",
    "    d = M.shape[0]\n",
    "\n",
    "    # Want top eigenvalue algebraically\n",
    "    eigenvalue, v = eigh(M, eigvals=(d-1,d-1)) \n",
    "    #eigenvalue, v = largest_eigsh(M, 1, which='LA',tol=tol)\n",
    "    #print(eigenvalue)\n",
    "    #print(np.sum(v ** 2))\n",
    "    \n",
    "    ## Don't regularize if constraint is satisfied\n",
    "    if eigenvalue > 0:\n",
    "        return v[:,0]\n",
    "    else:\n",
    "        return np.zeros(shape=v[:,0].shape)\n",
    "    \n",
    "# @jit(nopython=True)\n",
    "def a_step(a,X,resids_sq,v,lr,lam,eta,n):\n",
    "    \"\"\" Step to minimize \\sum_i a_i resids_sq[i]^2 + \\lambda sigma_{max}(X^T diag(1/n - a) X)\"\"\"\n",
    "    m = min(int(n*(1 - eta))+1,n) \n",
    "    \n",
    "    Xv_squared = np.dot(X,v)**2\n",
    "    penalties = resids_sq - lam * Xv_squared\n",
    "\n",
    "    #print(\"OBJ: \",np.dot(a,resids_sq), np.dot(a,-lam * Xv_squared) + lam * (1/n) * np.sum(Xv_squared))\n",
    "\n",
    "    #print(\"BEFORE: \",np.dot(a,penalties) + lam * (1/n) * np.sum(Xv_squared))\n",
    "\n",
    "    # multiplicative update\n",
    "    print('residuals: ',resids_sq.shape)\n",
    "    print('penalties: ', penalties.shape)\n",
    "    a *= np.exp(-lr * penalties)\n",
    "    a /= np.sum(a)\n",
    "    #print(\"MIDDLE:: \",np.dot(a,penalties) + lam * (1/n) * np.sum(Xv_squared))\n",
    "    # project back to sliced simplex\n",
    "    a = cap(a,m)\n",
    "    return a\n",
    "\n",
    "logistic_reg_oracle(x,y[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mw2 import MW_no_alt_min, get_weights, altmin_step\n",
    "\n",
    "def isotropic(Xs,fake=False):\n",
    "    if fake:\n",
    "        return Xs, np.eye(Xs.shape[1])\n",
    "    Sig = np.matmul(Xs.T,Xs)\n",
    "    Sig_sqrt = np.linalg.inv(sqrtm(Sig))\n",
    "    new_Xs = np.matmul(Xs,Sig_sqrt)\n",
    "    return new_Xs, Sig_sqrt\n",
    "\n",
    "def scram(x,y,params):\n",
    "    AM_steps = params[0]\n",
    "    altmin_params = params[1:]\n",
    "    N,d = x.shape\n",
    "    w = [0.]*d\n",
    "    a = [1.]*N\n",
    "    #iso_x, Sig_sqrt = isotropic(x)\n",
    "    iso_x = x\n",
    "    Sig_sqrt = np.eye(d)\n",
    "    AM_steps = 1\n",
    "    for i in range(AM_steps):\n",
    "        #print('AM step: ',i)\n",
    "        w,a = altmin_step(iso_x,y,a,altmin_params,init=False)\n",
    "    final_w = np.matmul(Sig_sqrt,w)    \n",
    "    return final_w\n",
    "\n",
    "#regression oracle \n",
    "def regression_oracle(x,y,mode='ols'):\n",
    "    if mode == 'ols':\n",
    "        model = LinearRegression()\n",
    "        model.fit(x,y)\n",
    "        regressor = model.coef_\n",
    "    if mode == 'scram':\n",
    "        lr = 0.5\n",
    "        lam = 0.2\n",
    "        MW_steps = 10\n",
    "        eta = 0.1\n",
    "        AM_steps = 10\n",
    "        params = (AM_steps,lr,lam,MW_steps,eta)\n",
    "        regressor = scram(x,y,params)\n",
    "    return regressor\n",
    "\n",
    "\n",
    "reg = regression_oracle(x,y[:,0])\n",
    "\n",
    "def optimal_reward(y):\n",
    "    a,b = y.shape\n",
    "    cumsum = 0\n",
    "    for i in range(a):\n",
    "        cumsum = cumsum + np.amax(y[i,:])\n",
    "    return cumsum/a\n",
    "\n",
    "max_reward = optimal_reward(y)\n",
    "print('Optimal Reward')\n",
    "print(max_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#contextual bandits takes covariates and labels\n",
    "def contextual_bandit(cov_label):\n",
    "    cov = cov_label['cov']\n",
    "    labels = cov_label['label']\n",
    "    (N,d) = cov.shape\n",
    "    (N,k) = labels.shape\n",
    "    estimators = np.zeros((k,d))\n",
    "    action_list = []\n",
    "    mu = k\n",
    "    delta = 0.1\n",
    "    gamma = np.sqrt(k*N/(d*np.log(N/d) + 1./(2*delta)))\n",
    "    params = (mu,gamma)\n",
    "    rewards = []\n",
    "    mean_reward = []\n",
    "    for i in range(N):\n",
    "        print('iteration: ',i)\n",
    "        covariate = cov[i,:]\n",
    "        values = np.zeros(k)\n",
    "        for j in range(k):\n",
    "            est = estimators[j,:]\n",
    "            values[j] = np.inner(est,covariate)\n",
    "        action = select_action(values,params)\n",
    "        action_list.append(action)\n",
    "        bandit_feedback = labels[i,action]\n",
    "        rewards.append(bandit_feedback)\n",
    "        (data_x,data_y) = get_data(cov_label,action_list,action) \n",
    "        \n",
    "        #bug, ols can run on one datapoint but scram can't\n",
    "        #estimators[action,:] = regression_oracle(data_x,data_y,mode='ols')\n",
    "        estimators[action,:] = regression_oracle(data_x,data_y,mode='scram')\n",
    "        print('action')\n",
    "        print(action)\n",
    "        print('average reward')\n",
    "        #print(rewards)\n",
    "        avg_reward = sum(rewards)/len(rewards)\n",
    "        mean_reward.append(avg_reward)\n",
    "        print(avg_reward)\n",
    "    return mean_reward\n",
    "\n",
    "def get_data(cov_label,action_list,action):\n",
    "    cov = cov_label['cov']\n",
    "    labels = cov_label['label']\n",
    "    (N,d) = cov.shape\n",
    "    (N,k) = labels.shape\n",
    "    length = len(action_list)\n",
    "    count = 0\n",
    "    for i in range(length):\n",
    "        if action_list[i] == action:\n",
    "            count = count + 1\n",
    "    \n",
    "    data_x = np.zeros((count,d))\n",
    "    data_y = np.zeros(count)\n",
    "    counter = 0\n",
    "    for i in range(len(action_list)):\n",
    "        if action_list[i] == action:\n",
    "            data_x[counter] = cov[i,:]\n",
    "            data_y[counter] = labels[i,action]\n",
    "            counter = counter + 1\n",
    "    return (data_x,data_y) \n",
    "\n",
    "\n",
    "def select_action(values,params):\n",
    "    (mu,gamma) = params\n",
    "    k = mu\n",
    "    max_value = np.amax(values)\n",
    "    max_index = np.where(values == max_value)[0][0]\n",
    "    prob = np.zeros(len(values))\n",
    "    for i in range(k): \n",
    "        if i == max_index:\n",
    "            next\n",
    "        else: \n",
    "            prob[i] = 1./(mu + gamma*(max_value - values[i]))\n",
    "    prob[max_index] = 1 - np.sum(prob)\n",
    "    prob = prob/np.sum(prob)\n",
    "    #print('probability')\n",
    "    #print(prob)\n",
    "    #TODO roulette wheel\n",
    "    draw = np.random.rand()\n",
    "    sums = 0\n",
    "    action = 0\n",
    "    for i in range(k):\n",
    "        sums = sums + prob[i]\n",
    "        if sums >= draw:\n",
    "            action = i\n",
    "            break\n",
    "    return action\n",
    "\n",
    "mean_reward = contextual_bandit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "domain = range(len(mean_reward))\n",
    "plt.scatter(domain, mean_reward)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[2,3],[1,4]])\n",
    "b = np.sort(a,axis=0)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([3,2,1])\n",
    "order = a.argsort()\n",
    "print(order)\n",
    "b = a[order]\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros((2,3))\n",
    "a.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([0.5,0.5,0.5])\n",
    "y = tf.constant([1.,0.,1.])\n",
    "weights = tf.constant([0.4,0.2,0.4])\n",
    "t = tf.ones(3)\n",
    "\n",
    "\n",
    "arg1 = tf.multiply(y,tf.math.log(x))\n",
    "arg2 = tf.multiply(tf.subtract(t,y),tf.math.log(tf.math.subtract(t,x)))\n",
    "q = tf.reduce_sum(tf.multiply(tf.add(arg1,arg2),weights))\n",
    "print(q)\n",
    "\n",
    "print(tf.math.scalar_mul(-1.0,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.ones(2)\n",
    "b = tf.ones(32)\n",
    "c = b.numpy()\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
