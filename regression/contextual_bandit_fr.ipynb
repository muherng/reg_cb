{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "# mosek only needed if we don't use MW\n",
    "#import mosek\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cProfile\n",
    "#from baselines import *\n",
    "from scipy.linalg import sqrtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#synthetic example with N datapoints, in d dimensions, with k actions, random regressors\n",
    "#returns a dictionary of covariates x, labels y, regressors ell.  \n",
    "def synthetic_example(N,d,k):\n",
    "    #covariates\n",
    "    x = np.random.normal(0,1,(N,d))\n",
    "    #regressors\n",
    "    ell = np.random.normal(0,1,(k,d))\n",
    "    #labels\n",
    "    y = np.zeros((N,k))\n",
    "    for i in range(N):\n",
    "        for j in range(k):\n",
    "            feature = x[i,:]\n",
    "            regressor = ell[j,:]\n",
    "            y[i,j] = np.inner(feature,regressor) + np.random.normal(0,1)\n",
    "    data = {'cov': x, 'label': y, 'reg': ell}\n",
    "    return data\n",
    "\n",
    "from scipy.special import logit, expit\n",
    "\n",
    "def logistic_synthetic_example(N,d,k,corrupt=False):\n",
    "    #covariates, scaling the covariates to be of large norm amplifies the effect of corruptions\n",
    "    x = np.random.normal(0,1,(N,d))\n",
    "    #regressors\n",
    "    ell = np.random.normal(0,1,(k,d))\n",
    "    ell = ell/np.linalg.norm(ell)\n",
    "    #labels\n",
    "    y = np.zeros((N,k))\n",
    "    prob_list = np.zeros((N,k))\n",
    "    for i in range(N):\n",
    "        for j in range(k):\n",
    "            feature = x[i,:]\n",
    "            regressor = ell[j,:]\n",
    "            prob = expit(np.inner(feature,regressor))\n",
    "            prob_list[i,j] = prob\n",
    "            y[i,j] = np.random.binomial(1,prob)\n",
    "            #deterministic labels\n",
    "            #if prob > 0.5:\n",
    "            #    y[i,j] = 1\n",
    "            #else:\n",
    "            #    y[i,j] = 0\n",
    "    corr_frac = 0.1\n",
    "    corr = int(corr_frac*N)\n",
    "    print('number of corruptions')\n",
    "    print(corr)\n",
    "    if corrupt:\n",
    "        select = np.zeros((corr,k))\n",
    "        for j in range(k):\n",
    "            order = np.argsort(prob_list[:,j])\n",
    "            select[:,j] = order[:corr]\n",
    "        for i in range(corr):\n",
    "            for j in range(k):\n",
    "                index = int(select[i,j])\n",
    "                y[index,j] = 1 - y[index,j]\n",
    "    data = {'cov': x, 'label': y, 'reg': ell}\n",
    "    return data\n",
    "\n",
    "#data = synthetic_example(1000,100,10)    \n",
    "#print(data)\n",
    "\n",
    "import pandas as pd, numpy as np, re\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "\n",
    "def parse_data(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        infoline = f.readline()\n",
    "        infoline = re.sub(r\"^b'\", \"\", str(infoline))\n",
    "        n_features = int(re.sub(r\"^\\d+\\s(\\d+)\\s\\d+.*$\", r\"\\1\", infoline))\n",
    "        features, labels = load_svmlight_file(f, n_features=n_features, multilabel=True)\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    labels = mlb.fit_transform(labels)\n",
    "    features = np.array(features.todense())\n",
    "    features = np.ascontiguousarray(features)\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "data_mode = 'logistic_synth'\n",
    "#data_mode = 'synthetic'\n",
    "#data_mode = 'real'\n",
    "N = 1000\n",
    "d = 100\n",
    "k = 2\n",
    "\n",
    "if data_mode == 'logistic_synth':\n",
    "    #data = logistic_synthetic_example(N,d,k,corrupt=True)\n",
    "    data = logistic_synthetic_example(N,d,k,corrupt=False)\n",
    "if data_mode == 'synthetic':\n",
    "    data = synthetic_example(1000,10,10)    \n",
    "elif data_mode == 'real': \n",
    "    x, y = parse_data(\"Bibtex_data.txt\")\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    data = {'cov': x, 'label': y}\n",
    "\n",
    "x = data['cov']\n",
    "y = data['label']\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(solver='liblinear', random_state=0, fit_intercept ='False')\n",
    "#a = np.zeros((2,3)).reshape(-1,1)\n",
    "#b = np.ones(2)\n",
    "\n",
    "#a = np.zeros(10,2).reshape(-1, 1)\n",
    "#b = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\n",
    "lab = y[:,0]\n",
    "model.fit(x, lab)\n",
    "print(model.coef_)\n",
    "print(data['reg'])\n",
    "print('score')\n",
    "print(model.score(x,lab))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression experiments\n",
    "#implement the weighted logistic regression\n",
    "import tensorflow as tf\n",
    "\n",
    "#sgd = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)\n",
    "#var = tf.Variable(2.5)\n",
    "\n",
    "#def func(x):\n",
    "#    return x**2\n",
    "\n",
    "#cost = lambda(func)(3)\n",
    "\n",
    "#for _ in range(100):\n",
    "#    sgd.minimize(cost, var_list=[var])\n",
    "\n",
    "#var.numpy()\n",
    "#cost().numpy()\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "#model = keras.Sequential()\n",
    "#model.add(layers.Dense(64, kernel_initializer='uniform', input_shape=(10,)))\n",
    "#model.add(layers.Activation('softmax'))\n",
    "\n",
    "#loss_function = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "#model.compile(loss=loss_function, optimizer='adam')\n",
    "\n",
    "#def custom_loss_function(y_true, y_pred):\n",
    "#   squared_difference = tf.square(y_true - y_pred)\n",
    "#   return tf.reduce_mean(squared_difference, axis=-1)\n",
    "\n",
    "#model.compile(optimizer='adam', loss=custom_loss_function)\n",
    "\n",
    "#y_true = [12, 20, 29., 60.]\n",
    "#y_pred = [14., 18., 27., 55.]\n",
    "#cl = custom_loss_function(np.array(y_true),np.array(y_pred))\n",
    "#cl.numpy()\n",
    "\n",
    "x_train = data['cov']\n",
    "y_train = y[:,0]\n",
    "\n",
    "#x_val = np.zeros((5,4))\n",
    "#y_val = np.array([0,0,1,1,1])\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import InputLayer\n",
    "\n",
    "ANN_model = Sequential()\n",
    "ANN_model.add(InputLayer(input_shape=(d, )))\n",
    "# No hidden layers\n",
    "ANN_model.add(Dense(1, activation='sigmoid'))\n",
    "ANN_model.summary()\n",
    "\n",
    "\n",
    "weights = tf.ones(d)\n",
    "\n",
    "def custom_loss_function(y_true, y_pred):\n",
    "    arg1 = y_true\n",
    "    arg2 = tf.math.log(y_pred)\n",
    "    arg3 = tf.subtract(tf.ones(d),y_true)\n",
    "    arg4 = tf.subtract(tf.ones(d),y_pred)\n",
    "    inp1 = tf.multiply(arg1,arg2)\n",
    "    inp2 = tf.multiply(arg3,arg4)\n",
    "    loss_vec = tf.math.scalar_mul(-1.0,tf.add(inp1,inp2))\n",
    "    weight_loss_vec = tf.multiply(weights,loss_vec)\n",
    "    output = tf.reduce_mean(weight_loss_vec, axis=-1)\n",
    "    #t = tf.square(tf.subtract(y_true,y_pred))\n",
    "    #output = tf.reduce_mean(t,axis=-1)\n",
    "\n",
    "    return output\n",
    "\n",
    "opt=tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "#loss=tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "#ANN_model.compile(optimizer=opt,\n",
    "#                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "#                  metrics=['accuracy'])\n",
    "\n",
    "ANN_model.compile(optimizer=opt,\n",
    "                  loss=custom_loss_function,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = ANN_model.fit(x_train, y_train, \n",
    "                        epochs=20, batch_size=32,\n",
    "                        validation_split=0.2, \n",
    "                        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mw2 import MW_no_alt_min, get_weights, altmin_step\n",
    "\n",
    "def isotropic(Xs,fake=False):\n",
    "    if fake:\n",
    "        return Xs, np.eye(Xs.shape[1])\n",
    "    Sig = np.matmul(Xs.T,Xs)\n",
    "    Sig_sqrt = np.linalg.inv(sqrtm(Sig))\n",
    "    new_Xs = np.matmul(Xs,Sig_sqrt)\n",
    "    return new_Xs, Sig_sqrt\n",
    "\n",
    "def scram(x,y,params):\n",
    "    AM_steps = params[0]\n",
    "    altmin_params = params[1:]\n",
    "    N,d = x.shape\n",
    "    w = [0.]*d\n",
    "    a = [1.]*N\n",
    "    #iso_x, Sig_sqrt = isotropic(x)\n",
    "    iso_x = x\n",
    "    Sig_sqrt = np.eye(d)\n",
    "    AM_steps = 1\n",
    "    for i in range(AM_steps):\n",
    "        #print('AM step: ',i)\n",
    "        w,a = altmin_step(iso_x,y,a,altmin_params,init=False)\n",
    "    final_w = np.matmul(Sig_sqrt,w)    \n",
    "    return final_w\n",
    "\n",
    "#regression oracle \n",
    "def regression_oracle(x,y,mode='ols'):\n",
    "    if mode == 'ols':\n",
    "        model = LinearRegression()\n",
    "        model.fit(x,y)\n",
    "        regressor = model.coef_\n",
    "    if mode == 'scram':\n",
    "        lr = 0.5\n",
    "        lam = 0.2\n",
    "        MW_steps = 10\n",
    "        eta = 0.1\n",
    "        AM_steps = 10\n",
    "        params = (AM_steps,lr,lam,MW_steps,eta)\n",
    "        regressor = scram(x,y,params)\n",
    "    return regressor\n",
    "\n",
    "\n",
    "reg = regression_oracle(x,y[:,0])\n",
    "\n",
    "def optimal_reward(y):\n",
    "    a,b = y.shape\n",
    "    cumsum = 0\n",
    "    for i in range(a):\n",
    "        cumsum = cumsum + np.amax(y[i,:])\n",
    "    return cumsum/a\n",
    "\n",
    "max_reward = optimal_reward(y)\n",
    "print('Optimal Reward')\n",
    "print(max_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#contextual bandits takes covariates and labels\n",
    "def contextual_bandit(cov_label):\n",
    "    cov = cov_label['cov']\n",
    "    labels = cov_label['label']\n",
    "    (N,d) = cov.shape\n",
    "    (N,k) = labels.shape\n",
    "    estimators = np.zeros((k,d))\n",
    "    action_list = []\n",
    "    mu = k\n",
    "    delta = 0.1\n",
    "    gamma = np.sqrt(k*N/(d*np.log(N/d) + 1./(2*delta)))\n",
    "    params = (mu,gamma)\n",
    "    rewards = []\n",
    "    mean_reward = []\n",
    "    for i in range(N):\n",
    "        print('iteration: ',i)\n",
    "        covariate = cov[i,:]\n",
    "        values = np.zeros(k)\n",
    "        for j in range(k):\n",
    "            est = estimators[j,:]\n",
    "            values[j] = np.inner(est,covariate)\n",
    "        action = select_action(values,params)\n",
    "        action_list.append(action)\n",
    "        bandit_feedback = labels[i,action]\n",
    "        rewards.append(bandit_feedback)\n",
    "        (data_x,data_y) = get_data(cov_label,action_list,action) \n",
    "        \n",
    "        #bug, ols can run on one datapoint but scram can't\n",
    "        #estimators[action,:] = regression_oracle(data_x,data_y,mode='ols')\n",
    "        estimators[action,:] = regression_oracle(data_x,data_y,mode='scram')\n",
    "        print('action')\n",
    "        print(action)\n",
    "        print('average reward')\n",
    "        #print(rewards)\n",
    "        avg_reward = sum(rewards)/len(rewards)\n",
    "        mean_reward.append(avg_reward)\n",
    "        print(avg_reward)\n",
    "    return mean_reward\n",
    "\n",
    "def get_data(cov_label,action_list,action):\n",
    "    cov = cov_label['cov']\n",
    "    labels = cov_label['label']\n",
    "    (N,d) = cov.shape\n",
    "    (N,k) = labels.shape\n",
    "    length = len(action_list)\n",
    "    count = 0\n",
    "    for i in range(length):\n",
    "        if action_list[i] == action:\n",
    "            count = count + 1\n",
    "    \n",
    "    data_x = np.zeros((count,d))\n",
    "    data_y = np.zeros(count)\n",
    "    counter = 0\n",
    "    for i in range(len(action_list)):\n",
    "        if action_list[i] == action:\n",
    "            data_x[counter] = cov[i,:]\n",
    "            data_y[counter] = labels[i,action]\n",
    "            counter = counter + 1\n",
    "    return (data_x,data_y) \n",
    "\n",
    "\n",
    "def select_action(values,params):\n",
    "    (mu,gamma) = params\n",
    "    k = mu\n",
    "    max_value = np.amax(values)\n",
    "    max_index = np.where(values == max_value)[0][0]\n",
    "    prob = np.zeros(len(values))\n",
    "    for i in range(k): \n",
    "        if i == max_index:\n",
    "            next\n",
    "        else: \n",
    "            prob[i] = 1./(mu + gamma*(max_value - values[i]))\n",
    "    prob[max_index] = 1 - np.sum(prob)\n",
    "    prob = prob/np.sum(prob)\n",
    "    #print('probability')\n",
    "    #print(prob)\n",
    "    #TODO roulette wheel\n",
    "    draw = np.random.rand()\n",
    "    sums = 0\n",
    "    action = 0\n",
    "    for i in range(k):\n",
    "        sums = sums + prob[i]\n",
    "        if sums >= draw:\n",
    "            action = i\n",
    "            break\n",
    "    return action\n",
    "\n",
    "mean_reward = contextual_bandit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "domain = range(len(mean_reward))\n",
    "plt.scatter(domain, mean_reward)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[2,3],[1,4]])\n",
    "b = np.sort(a,axis=0)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([3,2,1])\n",
    "order = a.argsort()\n",
    "print(order)\n",
    "b = a[order]\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros((2,3))\n",
    "a.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([0.5,0.5,0.5])\n",
    "y = tf.constant([1.,0.,1.])\n",
    "weights = tf.constant([0.4,0.2,0.4])\n",
    "t = tf.ones(3)\n",
    "\n",
    "\n",
    "arg1 = tf.multiply(y,tf.math.log(x))\n",
    "arg2 = tf.multiply(tf.subtract(t,y),tf.math.log(tf.math.subtract(t,x)))\n",
    "q = tf.reduce_sum(tf.multiply(tf.add(arg1,arg2),weights))\n",
    "print(q)\n",
    "\n",
    "print(tf.math.scalar_mul(-1.0,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
