{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "# mosek only needed if we don't use MW\n",
    "#import mosek\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cProfile\n",
    "#from baselines import *\n",
    "from scipy.linalg import sqrtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of corruptions\n",
      "100\n",
      "[[ 2.30787316e-02 -6.30942878e-02  4.62418812e-02  1.74071497e-01\n",
      "   3.18489379e-02  6.77065843e-02 -1.14898901e-02  5.02273144e-02\n",
      "  -9.46558449e-02  2.58239319e-02  9.49693188e-02  1.13307303e-01\n",
      "  -2.70775350e-02 -1.74725911e-01  2.50221875e-02 -5.58223868e-02\n",
      "  -2.28843613e-04  8.63528162e-02  1.01612888e-02 -1.23907154e-04\n",
      "   7.47641979e-02  3.84316740e-02  2.92284730e-02  4.35989853e-02\n",
      "   1.53887625e-02  1.27761256e-01 -3.71160828e-02  9.25371309e-02\n",
      "   7.41869407e-02 -2.33875481e-01  6.19099158e-02  5.95536163e-02\n",
      "   6.63133163e-02 -8.15397762e-02  8.20701088e-02 -8.35345300e-02\n",
      "   2.16182598e-01  7.70199801e-03 -1.59259419e-01  6.29760890e-02\n",
      "   4.34619795e-02 -4.84170555e-04  1.14247714e-01  1.05511981e-01\n",
      "   7.46061604e-02 -1.61608225e-01  2.45442804e-02  3.60100795e-02\n",
      "  -4.41368311e-02  5.99084406e-02 -2.19664605e-01 -1.25591587e-01\n",
      "  -1.81337222e-02 -5.81880075e-03 -6.57227470e-03  1.18047887e-02\n",
      "  -1.95437622e-02  9.25949379e-02  9.92068257e-02  5.94480692e-03\n",
      "  -7.75370697e-02  4.59306166e-02 -1.91066317e-02 -1.78880291e-02\n",
      "   3.38480230e-02 -1.58463856e-01  1.64238790e-01 -2.82383229e-02\n",
      "   6.10066090e-02  9.11242847e-02  4.69361733e-03 -4.17540492e-03\n",
      "   2.60883980e-02 -1.84978174e-02  4.65867927e-02  6.84626454e-02\n",
      "   1.41730743e-01 -3.34832121e-02 -8.22138539e-02  1.89340766e-02\n",
      "   1.91825729e-02 -7.11845643e-02  6.35063886e-02 -1.08600307e-02\n",
      "   1.24838197e-01 -5.68115394e-02  7.96922829e-02  8.73375929e-02\n",
      "   4.35817188e-02 -4.19410502e-03  1.58516129e-01 -1.04055860e-01\n",
      "   5.54834704e-02 -1.17240625e-02 -3.74705526e-02  3.38768144e-03\n",
      "   1.41321753e-03  3.08791813e-02  6.32159857e-02  6.68839371e-02]]\n",
      "[[ 1.61946331e-02 -1.36357376e-01  1.24570772e-01  7.44015372e-02\n",
      "  -7.83487905e-02  7.51578287e-02 -1.88553055e-02  1.24635066e-01\n",
      "   8.32878509e-03  2.11998496e-03  6.15499326e-02  3.45125204e-02\n",
      "  -6.44687471e-03 -1.25081721e-01 -2.90884904e-03  2.52577287e-02\n",
      "   3.37106229e-02  6.56132152e-02 -9.89204212e-02  2.83471402e-02\n",
      "   1.89393070e-03 -8.09042768e-02  4.48990880e-02 -7.98525225e-03\n",
      "  -6.44732002e-02  1.21856644e-02  2.80088756e-02  1.09617629e-01\n",
      "   7.88125094e-02 -1.06481198e-01 -1.74896126e-02  1.72420137e-02\n",
      "   1.14678394e-04  5.36604020e-02  4.55032824e-02 -1.19474878e-01\n",
      "   1.77017491e-01  2.28866708e-02 -4.73092127e-02 -1.59105582e-04\n",
      "   2.03124837e-02 -8.35762353e-05 -6.52187051e-03  5.00342265e-02\n",
      "   8.29341646e-02 -2.27571663e-01 -7.43784845e-02 -3.37431499e-03\n",
      "   2.57663318e-02 -4.67673930e-03 -9.63528557e-02 -8.21489606e-02\n",
      "  -3.15699217e-02 -4.65202303e-02  4.22726695e-02  6.45310910e-02\n",
      "  -8.61740665e-02  8.34187888e-02  6.37847945e-02  2.72769026e-02\n",
      "  -1.79431159e-02  1.43441213e-01  4.27909099e-03 -2.16875499e-02\n",
      "  -4.39869647e-02 -1.15115010e-01  1.26234889e-01  2.96258151e-04\n",
      "   6.69611051e-02  1.02489110e-01  7.05554535e-03  2.21289987e-02\n",
      "  -4.02185164e-02 -2.66967869e-02 -2.39097247e-02 -2.05551090e-02\n",
      "  -3.34867950e-02 -3.87119743e-02 -3.25454558e-02 -2.78724330e-03\n",
      "   2.74543144e-02  4.64331374e-02  5.68678451e-02 -5.80418684e-02\n",
      "   6.81668146e-02 -4.54970843e-02 -6.71584891e-02  2.95707616e-02\n",
      "  -2.87769998e-02  4.30385710e-02  1.60929936e-01 -1.03360893e-01\n",
      "  -1.70799570e-02 -2.77978691e-02  1.28641356e-02 -6.61315137e-02\n",
      "  -1.28918174e-01 -3.78892283e-02 -3.57523173e-03  4.42649047e-02]\n",
      " [ 2.73235352e-03 -1.88019675e-02 -6.92216556e-02 -8.65711401e-02\n",
      "  -1.16443807e-01  5.12760442e-02  3.80550306e-02 -1.02083032e-01\n",
      "   6.66922372e-03 -1.13420706e-01  3.27296619e-02 -2.69930730e-02\n",
      "   9.17279434e-03 -1.05081640e-01  2.14157030e-02 -1.10778667e-01\n",
      "  -2.42621595e-02  3.85878923e-02  1.42200171e-01 -2.15887450e-02\n",
      "  -5.38298928e-02 -2.51457489e-02 -3.04945675e-02  3.22936676e-02\n",
      "   1.06244451e-01 -2.20967945e-02 -1.89992601e-02 -1.24590054e-01\n",
      "   3.56613352e-02  7.73740293e-02 -1.64101545e-01 -1.25995126e-01\n",
      "   3.25538044e-02 -3.46257562e-02 -9.29370364e-02 -8.77280705e-02\n",
      "  -2.73065949e-02 -3.38425845e-02  7.48782725e-02 -1.19250348e-03\n",
      "  -3.40562733e-02  6.21966966e-02 -6.91173399e-02 -7.06876107e-02\n",
      "   3.85742292e-02 -6.09890640e-02  7.13196077e-02 -6.07750098e-02\n",
      "   6.28622311e-04  3.49099604e-02 -6.93976201e-02  8.39405710e-02\n",
      "   1.16826073e-02  2.72731002e-03 -5.39405422e-02  8.41618141e-02\n",
      "   1.49257409e-02  2.69677237e-03  7.56831851e-02  5.96720574e-03\n",
      "   1.33828765e-01 -2.51515936e-02  8.87939256e-02  2.86384161e-02\n",
      "  -4.07127657e-03 -2.83015585e-02  8.03325078e-02  7.22465015e-02\n",
      "  -1.05534146e-01  1.04921357e-02 -8.69226118e-02 -1.95781085e-02\n",
      "   7.17245117e-03  6.70714974e-03  5.46505460e-02  2.24645801e-02\n",
      "   1.02585707e-02 -5.67354796e-03  9.91153731e-02 -2.11051387e-01\n",
      "   7.72248479e-02 -5.52377359e-03  5.14887468e-02  7.54996803e-02\n",
      "   9.94562246e-02  9.68728188e-02  4.11851772e-03 -8.30999567e-02\n",
      "  -4.33004153e-02 -1.31630206e-01  2.52015029e-02 -4.04954609e-03\n",
      "   1.23799925e-01  4.64802226e-02 -8.38944152e-03 -1.56518378e-01\n",
      "  -1.17913922e-01 -5.70208266e-03  6.47118852e-03 -1.45738510e-01]]\n",
      "score\n",
      "0.63\n"
     ]
    }
   ],
   "source": [
    "#synthetic example with N datapoints, in d dimensions, with k actions, random regressors\n",
    "#returns a dictionary of covariates x, labels y, regressors ell.  \n",
    "def synthetic_example(N,d,k):\n",
    "    #covariates\n",
    "    x = np.random.normal(0,1,(N,d))\n",
    "    #regressors\n",
    "    ell = np.random.normal(0,1,(k,d))\n",
    "    #labels\n",
    "    y = np.zeros((N,k))\n",
    "    for i in range(N):\n",
    "        for j in range(k):\n",
    "            feature = x[i,:]\n",
    "            regressor = ell[j,:]\n",
    "            y[i,j] = np.inner(feature,regressor) + np.random.normal(0,1)\n",
    "    data = {'cov': x, 'label': y, 'reg': ell}\n",
    "    return data\n",
    "\n",
    "from scipy.special import logit, expit\n",
    "\n",
    "def logistic_synthetic_example(N,d,k,corrupt=False):\n",
    "    #covariates, scaling the covariates to be of large norm amplifies the effect of corruptions\n",
    "    x = np.random.normal(0,1,(N,d))\n",
    "    #regressors\n",
    "    ell = np.random.normal(0,1,(k,d))\n",
    "    ell = ell/np.linalg.norm(ell)\n",
    "    #labels\n",
    "    y = np.zeros((N,k))\n",
    "    prob_list = np.zeros((N,k))\n",
    "    for i in range(N):\n",
    "        for j in range(k):\n",
    "            feature = x[i,:]\n",
    "            regressor = ell[j,:]\n",
    "            prob = expit(np.inner(feature,regressor))\n",
    "            prob_list[i,j] = prob\n",
    "            y[i,j] = np.random.binomial(1,prob)\n",
    "            #deterministic labels\n",
    "            #if prob > 0.5:\n",
    "            #    y[i,j] = 1\n",
    "            #else:\n",
    "            #    y[i,j] = 0\n",
    "    corr_frac = 0.1\n",
    "    corr = int(corr_frac*N)\n",
    "    print('number of corruptions')\n",
    "    print(corr)\n",
    "    if corrupt:\n",
    "        select = np.zeros((corr,k))\n",
    "        for j in range(k):\n",
    "            order = np.argsort(prob_list[:,j])\n",
    "            select[:,j] = order[:corr]\n",
    "        for i in range(corr):\n",
    "            for j in range(k):\n",
    "                index = int(select[i,j])\n",
    "                y[index,j] = 1 - y[index,j]\n",
    "    data = {'cov': x, 'label': y, 'reg': ell}\n",
    "    return data\n",
    "\n",
    "#data = synthetic_example(1000,100,10)    \n",
    "#print(data)\n",
    "\n",
    "import pandas as pd, numpy as np, re\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "\n",
    "def parse_data(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        infoline = f.readline()\n",
    "        infoline = re.sub(r\"^b'\", \"\", str(infoline))\n",
    "        n_features = int(re.sub(r\"^\\d+\\s(\\d+)\\s\\d+.*$\", r\"\\1\", infoline))\n",
    "        features, labels = load_svmlight_file(f, n_features=n_features, multilabel=True)\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    labels = mlb.fit_transform(labels)\n",
    "    features = np.array(features.todense())\n",
    "    features = np.ascontiguousarray(features)\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "data_mode = 'logistic_synth'\n",
    "#data_mode = 'synthetic'\n",
    "#data_mode = 'real'\n",
    "N = 1000\n",
    "d = 100\n",
    "k = 2\n",
    "\n",
    "if data_mode == 'logistic_synth':\n",
    "    #data = logistic_synthetic_example(N,d,k,corrupt=True)\n",
    "    data = logistic_synthetic_example(N,d,k,corrupt=False)\n",
    "if data_mode == 'synthetic':\n",
    "    data = synthetic_example(1000,10,10)    \n",
    "elif data_mode == 'real': \n",
    "    x, y = parse_data(\"Bibtex_data.txt\")\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    data = {'cov': x, 'label': y}\n",
    "\n",
    "x = data['cov']\n",
    "y = data['label']\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(solver='liblinear', random_state=0, fit_intercept ='False')\n",
    "#a = np.zeros((2,3)).reshape(-1,1)\n",
    "#b = np.ones(2)\n",
    "\n",
    "#a = np.zeros(10,2).reshape(-1, 1)\n",
    "#b = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\n",
    "lab = y[:,0]\n",
    "model.fit(x, lab)\n",
    "print(model.coef_)\n",
    "print(data['reg'])\n",
    "print('score')\n",
    "print(model.score(x,lab))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 101\n",
      "Trainable params: 101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.1594 - accuracy: 0.4925 - val_loss: 0.0570 - val_accuracy: 0.4500\n",
      "Epoch 2/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0621 - accuracy: 0.5650 - val_loss: 0.0176 - val_accuracy: 0.4900\n",
      "Epoch 3/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.0214 - accuracy: 0.5763 - val_loss: -9.8201e-04 - val_accuracy: 0.5200\n",
      "Epoch 4/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: -0.0024 - accuracy: 0.6100 - val_loss: -0.0101 - val_accuracy: 0.5350\n",
      "Epoch 5/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: -0.0180 - accuracy: 0.6187 - val_loss: -0.0140 - val_accuracy: 0.5300\n",
      "Epoch 6/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: -0.0280 - accuracy: 0.6288 - val_loss: -0.0151 - val_accuracy: 0.5350\n",
      "Epoch 7/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: -0.0374 - accuracy: 0.6438 - val_loss: -0.0083 - val_accuracy: 0.5100\n",
      "Epoch 8/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: -0.0446 - accuracy: 0.6475 - val_loss: -0.0117 - val_accuracy: 0.5200\n",
      "Epoch 9/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: -0.0484 - accuracy: 0.6475 - val_loss: -0.0084 - val_accuracy: 0.5350\n",
      "Epoch 10/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: -0.0557 - accuracy: 0.6463 - val_loss: -0.0038 - val_accuracy: 0.5200\n",
      "Epoch 11/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: -0.0598 - accuracy: 0.6513 - val_loss: -0.0039 - val_accuracy: 0.5200\n",
      "Epoch 12/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: -0.0644 - accuracy: 0.6525 - val_loss: 0.0015 - val_accuracy: 0.5150\n",
      "Epoch 13/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: -0.0676 - accuracy: 0.6525 - val_loss: 0.0023 - val_accuracy: 0.5200\n",
      "Epoch 14/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: -0.0717 - accuracy: 0.6525 - val_loss: 0.0059 - val_accuracy: 0.5150\n",
      "Epoch 15/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: -0.0751 - accuracy: 0.6587 - val_loss: 0.0108 - val_accuracy: 0.5150\n",
      "Epoch 16/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: -0.0762 - accuracy: 0.6562 - val_loss: 0.0151 - val_accuracy: 0.5200\n",
      "Epoch 17/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: -0.0791 - accuracy: 0.6575 - val_loss: 0.0197 - val_accuracy: 0.5300\n",
      "Epoch 18/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: -0.0824 - accuracy: 0.6587 - val_loss: 0.0206 - val_accuracy: 0.5200\n",
      "Epoch 19/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: -0.0846 - accuracy: 0.6587 - val_loss: 0.0244 - val_accuracy: 0.5200\n",
      "Epoch 20/20\n",
      "25/25 [==============================] - 0s 2ms/step - loss: -0.0870 - accuracy: 0.6575 - val_loss: 0.0241 - val_accuracy: 0.5300\n"
     ]
    }
   ],
   "source": [
    "#logistic regression experiments\n",
    "#implement the weighted logistic regression\n",
    "import tensorflow as tf\n",
    "\n",
    "#sgd = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)\n",
    "#var = tf.Variable(2.5)\n",
    "\n",
    "#def func(x):\n",
    "#    return x**2\n",
    "\n",
    "#cost = lambda(func)(3)\n",
    "\n",
    "#for _ in range(100):\n",
    "#    sgd.minimize(cost, var_list=[var])\n",
    "\n",
    "#var.numpy()\n",
    "#cost().numpy()\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "#model = keras.Sequential()\n",
    "#model.add(layers.Dense(64, kernel_initializer='uniform', input_shape=(10,)))\n",
    "#model.add(layers.Activation('softmax'))\n",
    "\n",
    "#loss_function = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "#model.compile(loss=loss_function, optimizer='adam')\n",
    "\n",
    "#def custom_loss_function(y_true, y_pred):\n",
    "#   squared_difference = tf.square(y_true - y_pred)\n",
    "#   return tf.reduce_mean(squared_difference, axis=-1)\n",
    "\n",
    "#model.compile(optimizer='adam', loss=custom_loss_function)\n",
    "\n",
    "#y_true = [12, 20, 29., 60.]\n",
    "#y_pred = [14., 18., 27., 55.]\n",
    "#cl = custom_loss_function(np.array(y_true),np.array(y_pred))\n",
    "#cl.numpy()\n",
    "\n",
    "x_train = data['cov']\n",
    "y_train = y[:,0]\n",
    "\n",
    "#x_val = np.zeros((5,4))\n",
    "#y_val = np.array([0,0,1,1,1])\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import InputLayer\n",
    "\n",
    "ANN_model = Sequential()\n",
    "ANN_model.add(InputLayer(input_shape=(d, )))\n",
    "# No hidden layers\n",
    "ANN_model.add(Dense(1, activation='sigmoid'))\n",
    "ANN_model.summary()\n",
    "\n",
    "\n",
    "weights = tf.ones(d)\n",
    "\n",
    "def custom_loss_function(y_true, y_pred):\n",
    "    arg1 = y_true\n",
    "    arg2 = tf.math.log(y_pred)\n",
    "    arg3 = tf.subtract(tf.ones(d),y_true)\n",
    "    arg4 = tf.subtract(tf.ones(d),y_pred)\n",
    "    inp1 = tf.multiply(arg1,arg2)\n",
    "    inp2 = tf.multiply(arg3,arg4)\n",
    "    loss_vec = tf.math.scalar_mul(-1.0,tf.add(inp1,inp2))\n",
    "    weight_loss_vec = tf.multiply(weights,loss_vec)\n",
    "    output = tf.reduce_mean(weight_loss_vec, axis=-1)\n",
    "    #t = tf.square(tf.subtract(y_true,y_pred))\n",
    "    #output = tf.reduce_mean(t,axis=-1)\n",
    "\n",
    "    return output\n",
    "\n",
    "opt=tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "#loss=tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "#ANN_model.compile(optimizer=opt,\n",
    "#                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "#                  metrics=['accuracy'])\n",
    "\n",
    "ANN_model.compile(optimizer=opt,\n",
    "                  loss=custom_loss_function,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = ANN_model.fit(x_train, y_train, \n",
    "                        epochs=20, batch_size=32,\n",
    "                        validation_split=0.2, \n",
    "                        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mw2 import MW_no_alt_min, get_weights, altmin_step\n",
    "\n",
    "def isotropic(Xs,fake=False):\n",
    "    if fake:\n",
    "        return Xs, np.eye(Xs.shape[1])\n",
    "    Sig = np.matmul(Xs.T,Xs)\n",
    "    Sig_sqrt = np.linalg.inv(sqrtm(Sig))\n",
    "    new_Xs = np.matmul(Xs,Sig_sqrt)\n",
    "    return new_Xs, Sig_sqrt\n",
    "\n",
    "def scram(x,y,params):\n",
    "    AM_steps = params[0]\n",
    "    altmin_params = params[1:]\n",
    "    N,d = x.shape\n",
    "    w = [0.]*d\n",
    "    a = [1.]*N\n",
    "    #iso_x, Sig_sqrt = isotropic(x)\n",
    "    iso_x = x\n",
    "    Sig_sqrt = np.eye(d)\n",
    "    AM_steps = 1\n",
    "    for i in range(AM_steps):\n",
    "        #print('AM step: ',i)\n",
    "        w,a = altmin_step(iso_x,y,a,altmin_params,init=False)\n",
    "    final_w = np.matmul(Sig_sqrt,w)    \n",
    "    return final_w\n",
    "\n",
    "#regression oracle \n",
    "def regression_oracle(x,y,mode='ols'):\n",
    "    if mode == 'ols':\n",
    "        model = LinearRegression()\n",
    "        model.fit(x,y)\n",
    "        regressor = model.coef_\n",
    "    if mode == 'scram':\n",
    "        lr = 0.5\n",
    "        lam = 0.2\n",
    "        MW_steps = 10\n",
    "        eta = 0.1\n",
    "        AM_steps = 10\n",
    "        params = (AM_steps,lr,lam,MW_steps,eta)\n",
    "        regressor = scram(x,y,params)\n",
    "    return regressor\n",
    "\n",
    "\n",
    "reg = regression_oracle(x,y[:,0])\n",
    "\n",
    "def optimal_reward(y):\n",
    "    a,b = y.shape\n",
    "    cumsum = 0\n",
    "    for i in range(a):\n",
    "        cumsum = cumsum + np.amax(y[i,:])\n",
    "    return cumsum/a\n",
    "\n",
    "max_reward = optimal_reward(y)\n",
    "print('Optimal Reward')\n",
    "print(max_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#contextual bandits takes covariates and labels\n",
    "def contextual_bandit(cov_label):\n",
    "    cov = cov_label['cov']\n",
    "    labels = cov_label['label']\n",
    "    (N,d) = cov.shape\n",
    "    (N,k) = labels.shape\n",
    "    estimators = np.zeros((k,d))\n",
    "    action_list = []\n",
    "    mu = k\n",
    "    delta = 0.1\n",
    "    gamma = np.sqrt(k*N/(d*np.log(N/d) + 1./(2*delta)))\n",
    "    params = (mu,gamma)\n",
    "    rewards = []\n",
    "    mean_reward = []\n",
    "    for i in range(N):\n",
    "        print('iteration: ',i)\n",
    "        covariate = cov[i,:]\n",
    "        values = np.zeros(k)\n",
    "        for j in range(k):\n",
    "            est = estimators[j,:]\n",
    "            values[j] = np.inner(est,covariate)\n",
    "        action = select_action(values,params)\n",
    "        action_list.append(action)\n",
    "        bandit_feedback = labels[i,action]\n",
    "        rewards.append(bandit_feedback)\n",
    "        (data_x,data_y) = get_data(cov_label,action_list,action) \n",
    "        \n",
    "        #bug, ols can run on one datapoint but scram can't\n",
    "        #estimators[action,:] = regression_oracle(data_x,data_y,mode='ols')\n",
    "        estimators[action,:] = regression_oracle(data_x,data_y,mode='scram')\n",
    "        print('action')\n",
    "        print(action)\n",
    "        print('average reward')\n",
    "        #print(rewards)\n",
    "        avg_reward = sum(rewards)/len(rewards)\n",
    "        mean_reward.append(avg_reward)\n",
    "        print(avg_reward)\n",
    "    return mean_reward\n",
    "\n",
    "def get_data(cov_label,action_list,action):\n",
    "    cov = cov_label['cov']\n",
    "    labels = cov_label['label']\n",
    "    (N,d) = cov.shape\n",
    "    (N,k) = labels.shape\n",
    "    length = len(action_list)\n",
    "    count = 0\n",
    "    for i in range(length):\n",
    "        if action_list[i] == action:\n",
    "            count = count + 1\n",
    "    \n",
    "    data_x = np.zeros((count,d))\n",
    "    data_y = np.zeros(count)\n",
    "    counter = 0\n",
    "    for i in range(len(action_list)):\n",
    "        if action_list[i] == action:\n",
    "            data_x[counter] = cov[i,:]\n",
    "            data_y[counter] = labels[i,action]\n",
    "            counter = counter + 1\n",
    "    return (data_x,data_y) \n",
    "\n",
    "\n",
    "def select_action(values,params):\n",
    "    (mu,gamma) = params\n",
    "    k = mu\n",
    "    max_value = np.amax(values)\n",
    "    max_index = np.where(values == max_value)[0][0]\n",
    "    prob = np.zeros(len(values))\n",
    "    for i in range(k): \n",
    "        if i == max_index:\n",
    "            next\n",
    "        else: \n",
    "            prob[i] = 1./(mu + gamma*(max_value - values[i]))\n",
    "    prob[max_index] = 1 - np.sum(prob)\n",
    "    prob = prob/np.sum(prob)\n",
    "    #print('probability')\n",
    "    #print(prob)\n",
    "    #TODO roulette wheel\n",
    "    draw = np.random.rand()\n",
    "    sums = 0\n",
    "    action = 0\n",
    "    for i in range(k):\n",
    "        sums = sums + prob[i]\n",
    "        if sums >= draw:\n",
    "            action = i\n",
    "            break\n",
    "    return action\n",
    "\n",
    "mean_reward = contextual_bandit(data)\n",
    "#print('ground truth')\n",
    "#print(ell)\n",
    "#print('estimate')\n",
    "#print(est)\n",
    "#print('frobenius norm')\n",
    "#print(np.linalg.norm(ell - est))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "domain = range(len(mean_reward))\n",
    "plt.scatter(domain, mean_reward)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[2,3],[1,4]])\n",
    "b = np.sort(a,axis=0)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([3,2,1])\n",
    "order = a.argsort()\n",
    "print(order)\n",
    "b = a[order]\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros((2,3))\n",
    "a.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([0.5,0.5,0.5])\n",
    "y = tf.constant([1.,0.,1.])\n",
    "weights = tf.constant([0.4,0.2,0.4])\n",
    "t = tf.ones(3)\n",
    "\n",
    "\n",
    "arg1 = tf.multiply(y,tf.math.log(x))\n",
    "arg2 = tf.multiply(tf.subtract(t,y),tf.math.log(tf.math.subtract(t,x)))\n",
    "q = tf.reduce_sum(tf.multiply(tf.add(arg1,arg2),weights))\n",
    "print(q)\n",
    "\n",
    "print(tf.math.scalar_mul(-1.0,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
